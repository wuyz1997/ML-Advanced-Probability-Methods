{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2847fa55ab6e21bb9edebcd216ff3e12",
     "grade": false,
     "grade_id": "cell-5b335005bb36ae92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2021)\n",
    "\n",
    "Pekka Marttinen, Santosh Hiremath, Tianyu Cui, Yogesh Kumar, Zheyang Shen, Alexander Aushev, Khaoula El Mekkaoui, Shaoxiong Ji, Alexander Nikitin, Sebastiaan De Peuter, Joakim Järvinen.\n",
    "\n",
    "## Exercise 5, due on Tuesday February 23 at 23:00.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: EM for missing observations\n",
    "2. Problem 2: Extension of 'simple example' from the lecture\n",
    "3. Problem 3: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18c12b98afa6a333b6b4717029202b7d",
     "grade": false,
     "grade_id": "cell-298bb2ed1de6d806",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: EM for missing observations\n",
    "Suppose random variables $X_{i}$ follow a bivariate normal distribution $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$, where\n",
    "$ \\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix} $.\n",
    "\n",
    "Suppose further that we have observations on $X_{1}=(X_{11},X_{12})^{T}$, $X_{2}=(X_{21},X_{22})^{T}$ and $X_{3}=(X_{31},X_{32})^{T}$, such that\n",
    "$X_{1}$ and $X_{3}$ are fully observed, and from $X_{2}$ we have observed only\n",
    "the second coordinate. Thus, our data matrix can be written as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12}\\\\\n",
    "? & x_{22}\\\\\n",
    "x_{31} & x_{32}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "where the rows correspond to the transposed observations $\\mathbf{x}_{1}^{T},\\mathbf{x}_{2}^{T},\\mathbf{x}_{3}^{T}$. Suppose we want to learn the unknown parameter $\\rho$ using the EM-algorithm. Denote the missing observation by $Z$ and derive the E-step of the algorithm, i.e., __(a)__ write the complete data log-likelihood $\\ell(\\rho)$, __(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$, and __(c)__ evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "1. In general, for $X \\sim \\mathcal{N}_2(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $X=(X_1, X_2)^{T}$, $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2)^{T}$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix}$, \n",
    "we have \n",
    "$$ X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right),$$  with $\\rho$ being the correlation coefficient.\n",
    "2. For evaluating the expectation of $\\ell(\\rho)$, you can make use of the following two rules: \n",
    "    - $\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2} = trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T}).$\n",
    "    - if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $\\langle{X^2}\\rangle = \\mu^2 + \\sigma^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __(a)__ write the complete data log-likelihood\n",
    "$$\\begin{aligned}\n",
    "\\ell(\\rho)&=\\log \\prod_{i=1}^{3} p\\left(x_{i} \\mid \\rho\\right)\\\\\n",
    "&=\\sum_{i=1}^{3}\\log  p\\left(x_{i} \\mid \\rho\\right)\\\\\n",
    "&=-\\frac{1}{2}x_1^{T}\\Sigma^{-1}x_{1}-\\frac{1}{2}x_2^T\\Sigma^{-1}x_{2}-\\frac{1}{2}x_3^T\\Sigma^{-1}x_{3}-\\frac{3}{2}\\log{(1-\\rho^2})-3\\log(2\\pi)\n",
    "\\end{aligned}$$\n",
    "__(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$\n",
    "According to the Hint 1, we have\n",
    "$$\\begin{aligned}\n",
    "P(Z|x_{22})&\\sim \\mathcal{N}\\left(Z|0+ \\frac{1}{1}\\rho (x_{22}-0),(1-\\rho^2)1^2 \\right)\\\\\n",
    "&=\\mathcal{N}\\left(Z|\\rho x_{22},(1-\\rho^2) \\right)\\\\\n",
    "&=\\frac{1}{\\sqrt{2\\pi (1-\\rho^2)}}\\exp{\\left( -\\frac{Z-\\rho x_{22}}{2\\ (1-\\rho^2)}\\right)}.\n",
    "\\end{aligned}$$\n",
    "Therefore,\n",
    "$$\\begin{aligned}\n",
    "E_{Z \\mid X, \\rho_{0}}(Z)&=\\rho x_{22}\\\\\n",
    "E_{Z \\mid X, \\rho_{0}}(Z^2)&=1-\\rho^2+\\rho^2 x_{22}^2\n",
    "\\end{aligned}$$\n",
    "__(c)__ evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "According to the Hint 1, we have\n",
    "$$\\begin{aligned}\n",
    "Q(\\rho,\\rho_0)&=E_{Z \\mid X, \\rho_{0}}\\left(\\ell(\\rho)\\right)\\\\\n",
    "&=E_{Z \\mid X, \\rho_{0}}\\left(-\\frac{1}{2}x_1^{T}\\Sigma^{-1}x_{1}-\\frac{1}{2}x_2^T\\Sigma^{-1}x_{2}-\\frac{1}{2}x_3^T\\Sigma^{-1}x_{3}-\\frac{3}{2}\\log{(1-\\rho^2})-3\\log(2\\pi)\\right)\\\\\n",
    "&=-\\frac{1}{2}E_{Z \\mid X, \\rho_{0}}\\left(x_2^T\\Sigma^{-1}x_{2}\\right)-\\frac{3}{2}\\log{(1-\\rho^2)}-3\\log(2\\pi)-\\frac{1}{2}x_1^T\\Sigma^{-1}x_{1}-\\frac{1}{2}x_3^T\\Sigma^{-1}x_{3}\\\\\n",
    "\\text{By using hint 2,}\\\\\n",
    "&=-\\frac{1}{2}E_{Z \\mid X, \\rho_{0}}\\left(trace\\left(\\Sigma^{-1}x_{2}x_2^T\\right)\\right)-\\frac{3}{2}\\log{(1-\\rho^2})-3\\log(2\\pi)-\\frac{1}{2}trace\\left(\\Sigma^{-1}x_{1}x_1^T\\right)-\\frac{1}{2}trace\\left(\\Sigma^{-1}x_{3}x_3^T\\right)\\\\\n",
    "\\text{The first term}\\\\\n",
    " &=-\\frac{1}{2}trace\\left(\n",
    "\\left[\\begin{array}{ll}\n",
    "1 & \\rho \\\\\n",
    "\\rho & 1\n",
    "\\end{array}\\right]^{-1}\n",
    "\\left[\\begin{array}{cc}\n",
    "E_{Z \\mid X, \\rho_{0}}\\left(Z^{2}\\right) & E_{Z \\mid X, \\rho_{0}}(Z) x_{22} \\\\\n",
    "E_{Z \\mid X, \\rho_{0}}(Z) x_{22} & x_{22}^{2}\n",
    "\\end{array}\\right]\\right)\n",
    "\\\\\n",
    "&=-\\frac{1}{2}trace\\left(\n",
    "\\frac{1}{1-\\rho^{2}}\\left[\\begin{array}{cc}\n",
    "1 & -\\rho \\\\\n",
    "-\\rho & 1\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{cc}\n",
    "\\rho_0^{2} x_{22}^{2}+1-\\rho_0^{2} & \\rho_0 x_{22}^{2} \\\\\n",
    "\\rho_0 x_{22}^{2} & x_{22}^{2}\n",
    "\\end{array}\\right]\\right)\\\\\n",
    "&=-\\frac{1}{2}\\left(\\frac{1-\\rho_0^2-\\rho_0^2x_{22}^2+x_{22}^2-2\\rho\\rho_0x_{22}^2}{1-\\rho^2}\\right).\\\\\n",
    "\\text{Then the fourth term }\\\\\n",
    "&=-\\frac{1}{2}trace\\left(\n",
    "\\frac{1}{1-\\rho^{2}}\\left[\\begin{array}{cc}\n",
    "1 & -\\rho \\\\\n",
    "-\\rho & 1\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{cc}\n",
    "x_{11}^2 & x_{11}x_{12} \\\\\n",
    "x_{12}x_{11} & x_{12}^{2}\n",
    "\\end{array}\\right]\\right)\\\\\n",
    "&=-\\frac{1}{2-2\\rho^2}\\left(x_{11}^2-2\\rho x_{11}x_{12}+x_{12}^2\\right).\\\\\n",
    "\\text{Similarly, the fifth term}\\\\\n",
    "&=-\\frac{1}{2-2\\rho^2}\\left(x_{31}^2-2\\rho x_{31}x_{32}+x_{32}^2\\right).\\\\\n",
    "\\text{Therefore, }\\\\\n",
    "Q(\\rho,\\rho_0)&=-\\frac{1}{2-2\\rho^2}\\left(1-\\rho_0^2-\\rho_0^2x_{22}^2+x_{22}^2-2\\rho\\rho_0x_{22}^2+x_{11}^2-2\\rho x_{11}x_{12}+x_{12}^2+x_{31}^2-2\\rho x_{31}x_{32}+x_{32}^2\\right)+c,\\\\\n",
    "\\text{where } c=-\\frac{3}{2}\\log{(1-\\rho^2})-3\\log(2\\pi).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1cd36c403dde3a532a877a43ad92522",
     "grade": false,
     "grade_id": "cell-46bf29d7d4d92271",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Extension of 'simple example' from the lecture\n",
    "Suppose that we have $N$ independent observations $x = ( x_1, \\dots, x_N )$ from a two-component mixture of univariate Gaussian distributions with unknown mixing co-efficients and unknown mean of the second component:\n",
    "$$ p(x_{n} \\mid \\theta,\\tau)=(1-\\tau)\\mathcal{N}(x_{n}|0,1)+\\tau\\mathcal{N}(x_{n} \\mid \\theta,1).$$\n",
    "\n",
    "**(a)** Write down the complete data log-likelihood and derive the EM-algorithm for learning the maximum likelihood estimates for $\\theta$ and $\\tau$. \n",
    "\n",
    "**(b)** Simulate some data from the model ($N = 100$ samples) with the true values of parameters $\\theta$ = 3 and $\\tau = 0.5$. Run your EM algorithm to see whether the learned parameters converge close to the true values (by e.g. just listing the estimates from a few iterations or plotting them). Use the code template below (after the answer cell) as a starting point. \n",
    "\n",
    "**HINT**: The E and M steps for simple example.pdf from the lecture material looks as follows\n",
    "```Python\n",
    "\t# E-step: compute the responsibilities r2 for component 2\n",
    "\tr1_unnorm = scipy.stats.norm.pdf(x, 0, 1)\n",
    "\tr2_unnorm = scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "\tr2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\t\n",
    "\t# M-step: compute the parameter value that maximizes\n",
    "\t# the expectation of the complete-data log-likelihood.\n",
    "\ttheta[it] = sum(r2 * x) / sum(r2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 2(a) here\n",
    "We introduce unobersrved **latent variables** $Z=\\{z_n\\}_{n=1}^N$,\n",
    "$\n",
    "z_{n}=\\left(z_{n 1}, z_{n 2}\\right)^{T}=\\left\\{\\begin{array}{ll}\n",
    "(1,0)^{T}, & \\text{if} \\ x_{n}\\sim N\\left(x_{n} \\mid 0,1\\right) \\\\\n",
    "(0,1)^{T}, & \\text{if} \\ x_{n}\\sim N\\left(x_{n} \\mid \\theta,1\\right)\n",
    "\\end{array}\\right.\n",
    "$\n",
    "\n",
    "Hence\n",
    "$$\\begin{aligned}\n",
    "\\log(p(X,Z|\\theta,\\tau)&=\\log\\left(\\prod_{n=1}^{N}p(x_n,z_n|\\theta,\\tau) \\right)\\\\\n",
    "&=\\sum_{n=1}^{N}\\log\\left(p(x_n,z_n|\\theta,\\tau) \\right)\\\\\n",
    "&=\\sum_{n=1}^{N}\\log\\left(\\left((1-\\tau)\\mathcal{N}(x_n|0,1)\\right)^{z_{n1}}+\\left(\\tau\\mathcal{N}(x_n|\\theta,1)\\right)^{z_{n2}}\\right)\\\\\n",
    "&=\\sum_{n=1}^{N}z_{n1}\\log(1-\\tau)+z_{n0}\\log\\tau+z_{n1}\\log(\\mathcal{N}(x_n|0,1))+z_{n2}\\log(\\mathcal{N}(x_n|\\theta,1))\n",
    "\\end{aligned}$$\n",
    "## Derive the EM-algorithm\n",
    "### E step\n",
    "\\begin{aligned}\n",
    "\\gamma(z_{n2})&=p\\left(z_{n 2}=1\\mid\\mathbf{x}_{n},\\theta_0,\\rho_0\\right)&=\\frac{\\tau_{0} \\mathcal{N}\\left(x_{n} \\mid \\theta_{0}, 1\\right)}{\\left(1-\\tau_{0}\\right) \\mathcal{N}\\left(x_{n} \\mid 0,1\\right)+\\tau_{0} \\mathcal{N}\\left(x_{n} \\mid \\theta_{0}, 1\\right)}.\\\\ \n",
    "\\end{aligned}\n",
    "Then we compute the expectation of the complete log-likelihood over the posterior distribution of the latent variables:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q\\left(\\theta, \\tau, \\theta_{0}, \\tau_{0}\\right)&=E_{z \\mid x, \\theta_{0}, \\tau_{0}}(\\log p(x, z \\mid \\theta,\\tau))\\\\\n",
    "&=\\sum_{n=1}^{N}\\left(E_{z \\mid x, \\theta_{0}, \\tau_{0}}\\left(z_{n 1}\\right) \\log \\mathcal{N}\\left(x_{n} \\mid 0,1\\right)+E_{z \\mid x, \\theta_{0}, \\tau_{0}}\\left(z_{n 2}\\right) \\log \\mathcal{N}\\left(x_{n} \\mid \\theta, 1\\right)\\right.\\\\&+\n",
    "\\left.E_{z \\mid x, \\theta_{0}, \\tau_{0}}\\left(z_{n 1} \\log (1-\\tau)\\right)+E_{z \\mid x, \\theta_{0}, \\tau_{0}}\\left(z_{n 2} \\log \\tau\\right)\\right)\\\\\n",
    "&=\\sum_{n=1}^N\\left(1-\\gamma\\left(z_{n 2}\\right)\\right) \\log \\mathcal{N}\\left(x_{n} \\mid 0,1\\right)+\\gamma\\left(z_{n 2}\\right) \\log \\mathcal{N}\\left(x_{n} \\mid \\theta, 1\\right) \\\\&+\\left(1-\\gamma\\left(z_{n 2}\\right)\\right) \\log (1-\\tau)+\\gamma\\left(z_{n 2}\\right) \\log \\tau.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### M step\n",
    "Firstly, we evulate $\\theta$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{d \\theta} Q\\left(\\theta, \\tau, \\theta_{0}, \\tau_{0}\\right)&=\\frac{d}{d \\theta}\\left(\\gamma\\left(z_{n 2}\\right) \\log \\mathcal{N}\\left(x_{n} \\mid \\theta, 1\\right)\\right)\\\\\n",
    "&=\\sum_{n=1}^{N} \\gamma\\left(z_{n 2}\\right)\\left(x_{n}-\\theta\\right)\n",
    "\\end{aligned}\n",
    "Hence\n",
    "$$\n",
    "\\frac{d}{d \\theta} Q\\left(\\theta, \\tau, \\theta_{0}, \\tau_{0}\\right)=\\sum_{n=1}^{N} \\gamma\\left(z_{n 2}\\right)\\left(x_{n}-\\theta\\right)=0,\n",
    "$$\n",
    "and thus\n",
    "$$\n",
    "\\theta^{\\text {new }}=\\frac{\\sum_{n=1}^{N} \\gamma\\left(z_{n 2}\\right) x_{n}}{\\sum_{n=1}^{N} \\gamma\\left(z_{n 2}\\right)}.\n",
    "$$\n",
    "Secondly, we evulate $\\gamma$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{d \\tau} Q\\left(\\theta, \\tau, \\theta_{0}, \\tau_{0}\\right)&=\\frac{d}{d \\tau}\\left(\\left(1-\\gamma\\left(z_{n 2}\\right)\\right) \\log (1-\\tau)+\\gamma\\left(z_{n 2}\\right) \\log \\tau\\right)\\\\\n",
    "&=\\sum_{n=1}^{N}\\frac{\\gamma(z_{n2})-1}{1-\\tau}+\\frac{\\gamma(z_{n2})}{\\tau}\\\\\n",
    "&=\\sum_{n=1}^{N}\\frac{\\gamma(z_{n2})-\\tau}{(1-\\tau)\\tau}\\\\\n",
    "&=\\frac{-N\\tau+\\sum_{n=1}^{N}\\gamma(z_{n2})}{N(1-\\tau)\\tau}\n",
    "\\end{aligned}\n",
    "Hence $$\\frac{d}{d \\tau} Q\\left(\\theta, \\tau, \\theta_{0}, \\tau_{0}\\right)=\\frac{-N\\tau+\\sum_{n=1}^{N}\\gamma(z_{n2})}{N(1-\\tau)\\tau}=0,$$\n",
    "and thus\n",
    "$$\n",
    "\\tau^{\\text{new}}=\\frac{\\sum_{n=1}^{N}\\gamma(z_{n2})}{N}.\n",
    "$$\n",
    "In detail, $\\sum_{n=1}^{N}\\gamma(z_{n2})$ can be interpreted as the number of obeservations to the second component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b479fd715d72b0417e4f3ac4c0f8e914",
     "grade": false,
     "grade_id": "cell-1abac854e88e7dc1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta       tau\n",
      "1.0000000  0.1000000\n",
      "3.2393002  0.3798055\n",
      "3.2787207  0.5396835\n",
      "3.2208416  0.5618515\n",
      "3.2012856  0.5684228\n",
      "3.1949151  0.5705275\n",
      "3.1928396  0.5712102\n",
      "3.1921631  0.5714324\n",
      "3.1919426  0.5715048\n",
      "3.1918707  0.5715284\n",
      "3.1918472  0.5715361\n",
      "3.1918396  0.5715386\n",
      "3.1918371  0.5715394\n",
      "3.1918363  0.5715397\n",
      "3.1918360  0.5715397\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfL0lEQVR4nO3df3TcdZ3v8ec7yaRJ05SWJpT0B6Rwyg8La4G0/NIVd8VDOVVWt1dBrih4ZVGxsu6euz14D+Je9xxWXK+iK4W9chGt6N2iiD3FX7twEQQhqaGkFLUglTShTdNm2jRJ8+t9//h+J51OJ82kmWQy3+/rcc6c+c73+5mZd76dvvLNZz7fz9fcHRERKX4lhS5ARETyQ4EuIhIRCnQRkYhQoIuIRIQCXUQkIsoK9cY1NTVeX19fqLcXESlKTU1Ne929Ntu2ggV6fX09jY2NhXp7EZGiZGY7R9umLhcRkYhQoIuIRIQCXUQkIhToIiIRoUAXEYkIBbqISEQo0EVEIqJg49CLTbJngFf3dvNaxyH2HOzjnFOruWDxXOZWlRe6NBERQIF+lIGhYd7Y18NrHYd4LQzv1zoO8WpHN52H+rM+54zaKi48bS4XnT6XC0+by9JTZlFSYlNcuYhITAO9q6efHXuCwE4ddb/a0c2fOnsYHD5ywY95VeWcUVvFu86dzxm1VZxRO4sza6uoqZ7By20HaNq5n9/+aT//sX03G5taAaiuKGP54jkjAb/8tDnMrkgU6kcVkRixQl2xqKGhwQtx6n/Tzv38l/W/JpXbiVKjfl7VSGCfUXMkuOfMzK07xd15vbOHpp372fKn/WzZuZ/f7T6IO5jB2fOrueC0uVx4WhD0S2qqMNNRvIiMn5k1uXtDtm2xO0J/uS3JsMM9113Any08iUVzKykrndh3w2bGkpoqltRUseaiRQAc7Bug+Y0utuzsoulP+9m0tY2Hn/8TALMryjhldgVzZyaYO7M8uFWVB4+rgscnVyWYE247qTJBqbpxRGQMsQv0tmQfiVJj9fl1k9rXXV2R4O1La3n70mBStOFhZ0dHN1t27qelLUlndz/7e/rZ2dlD8xtd7O/pZ2Ao+19LZnBSZYKTZ5YzZ2YQ9DPKSigvKyFRGtyXl5aQKLUs6460S5QaM8pKKCspobTEKCkxSgxKzLDwPrgFv6RK0taNbC8J26fVRvjIjLT14bqRNmAYo/1hkrk+8y+YzKdN9A8cO+YVi4v+wCtuM8tLqZ6Ertj4BXpXL/NnV0z5F5clJcZZ86s5a3511u3uzqH+IfYfCoJ+f8/AkeVDweN9Pf109fSz+0Af/YPD9A8NMzA4TP+Q0z84xMCQMzA0fNT3ACIy/dzyjjNZt+qcvL9u7AK9vauPBXMqC13GMcyMWTPKmDWjjMUnz5zQaw0NB8GeCvyBIT/yC2BomP7BYYbdGfbgF8mwEz52fGSZ8LEzPMxR7YfC713cIfWrI/27mNSi40eWR2kbtMvgmQ8z2k/w91Wx/7or0Ndekkfn1mU/sJuo2AX6rq5eVtTPLXQZk6q0xCgtKaUiUVroUkRkCsXqTNGhYWf3gel5hC4iMlGxCvSOg4cZHHbqFOgiEkGxCvS2ZC8AC+dUFLgSEZH8i1egdwWBXneSjtBFJHpiFejtXX0A6kMXkUiKVaDv6uqlqryU2RWxG9wjIjEwZqCbWYWZPW9mL5rZNjP7QpY2Zmb3mNkOM9tqZhdOTrkT057sZcGcSs2jIiKRlMuh6mHgL9y928wSwNNm9ri7P5fWZhWwNLxdDNwb3k8r7ck+jXARkcga8wjdA93hw0R4yzxX7RrgobDtc8AcM6vLb6kT19bVy4KTNMJFRKIppz50Mys1s2ZgD/ALd/9NRpOFwBtpj1vDddNG38AQe7v79YWoiERWToHu7kPuvhxYBKw0s/MymmTrlD5mxgkzu9nMGs2ssaOjY9zFTsSbyWCES52O0EUkosY1ysXdu4AngasyNrUCi9MeLwLasjz/fndvcPeG2tra8VU6QUdOKtIRuohEUy6jXGrNbE64XAm8C3glo9ljwA3haJdLgKS7t+e72IloC8eg60tREYmqXEa51AHfNrNSgl8A/9fdN5nZLQDuvh7YDFwN7AB6gBsnqd4T1j5ylqi6XEQkmsYMdHffClyQZf36tGUHPpXf0vKrLdnLvKpyTSkrIpEVmzNF26bphS1ERPIlNoHenuxVd4uIRFpsAl1H6CISdbEI9AN9A3QfHmSB5kEXkQiLRaBrHnQRiYNYBLrmQReROIhFoO8Kj9DV5SIiURaLQG9P9lJaYpxSrUAXkeiKRaC3dfVx6uwKSkt0YQsRia6YBHqvultEJPLiEejJXo1wEZHIi3ygDw87byZ1UpGIRF/kA33vocMMDLm6XEQk8iIf6CPzoKvLRUQiLvKB3q4x6CISE5EP9JGTinSELiIRF/lAb0/2UZkoZc7MRKFLERGZVJEP9LauXurmVGCmk4pEJNqiH+jJPhZqyKKIxED0A71LVyoSkXiIdKD3Dw6zt/uwTioSkViIdKDvPtCHu0a4iEg8RDrQU0MW6zQGXURiYMxAN7PFZvaEmW03s21m9pksba4ws6SZNYe3Oyan3PFpT6ZOKtIRuohEX1kObQaBv3P3LWZWDTSZ2S/c/eWMdr9y99X5L/HEpU77V5eLiMTBmEfo7t7u7lvC5YPAdmDhZBeWD21dvcydmaCyvLTQpYiITLpx9aGbWT1wAfCbLJsvNbMXzexxM1s2yvNvNrNGM2vs6OgYf7XjFAxZ1NG5iMRDzoFuZrOAR4Db3P1AxuYtwOnu/lbg68Cj2V7D3e939wZ3b6itrT3BknPXrnnQRSRGcgp0M0sQhPkGd/9h5nZ3P+Du3eHyZiBhZjV5rfQE7NKl50QkRnIZ5WLAt4Dt7v6VUdqcGrbDzFaGr9uZz0LH62DfAAf7BnWELiKxkcsol8uBDwMvmVlzuO524DQAd18PrAE+YWaDQC9wrbt7/svNXXsydWELHaGLSDyMGeju/jRw3KkK3f0bwDfyVVQ+tHVpDLqIxEtkzxRNHaEr0EUkLiIb6G1dvZQYzK+eUehSRESmRIQDvY/5sysoK43sjygicpTIpp3mQReRuIlsoLcne9V/LiKxEslAd3fadJaoiMRMJAO981A//YPDLFCXi4jESCQDvW3kwhY6QheR+IhooAdj0Bcq0EUkRiIZ6KkrFWmUi4jESSQDva2rlxllJZxcVV7oUkREpkw0Az0c4RJOACkiEgvRDHSdVCQiMRTJQG/v0hh0EYmfyAX6wNAwuw/2aQy6iMRO5AJ994E+3DVtrojET+QCPTUGXScViUjcRC7QU2PQF+ri0CISM5EL9F2p0/5P0hG6iMRL5AK9vauP2RVlVM3I5frXIiLREb1A1zzoIhJTkQv0XRqDLiIxNWagm9liM3vCzLab2TYz+0yWNmZm95jZDjPbamYXTk65YwuO0PWFqIjETy4dzYPA37n7FjOrBprM7Bfu/nJam1XA0vB2MXBveD+levoH6eoZ0BeiIhJLYx6hu3u7u28Jlw8C24GFGc2uAR7ywHPAHDOry3u1Y9A86CISZ+PqQzezeuAC4DcZmxYCb6Q9buXY0MfMbjazRjNr7OjoGGepYxu5UpFO+xeRGMo50M1sFvAIcJu7H8jcnOUpfswK9/vdvcHdG2pra8dXaQ5SJxXpS1ERiaOcAt3MEgRhvsHdf5ilSSuwOO3xIqBt4uWNz66uPszgVB2hi0gM5TLKxYBvAdvd/SujNHsMuCEc7XIJkHT39jzWmZP2rl5qZ80gURq50ZgiImPKZZTL5cCHgZfMrDlcdztwGoC7rwc2A1cDO4Ae4Ma8V5qD9qTGoItIfI0Z6O7+NNn7yNPbOPCpfBV1otq6ejmnrrrQZYiIFERk+ibcnbZkLws0Bl1EYioygb6/Z4C+gWHNgy4isRWZQE+NQdc86CISV5ELdJ32LyJxFZlAb08Gp/1rlIuIxFVkAr2tq5fy0hLmVZUXuhQRkYKITqAn+6ibU0FJyXFHWIqIRFZ0Ar2rV5NyiUisRSbQ27s0Bl1E4i0SgT44NMzug4f1haiIxFokAn3PwcMMDTt1GoMuIjEWiUDXPOgiIhEJ9F3hpefUhy4icRaJQG/vSh2hq8tFROIrEoHe1tVL9YwyqisShS5FRKRgohHourCFiEhEAr2rVyNcRCT2IhHo7ck+zbIoIrFX9IHeNzDEvkP9mgddRGKv6ANd86CLiASKPtA1D7qISKDoA32XxqCLiAA5BLqZPWBme8ysZZTtV5hZ0syaw9sd+S9zdO3hWaKnaupcEYm5shzaPAh8A3joOG1+5e6r81LROLV19VIzawYzykoL8fYiItPGmEfo7v4UsG8KajkhbclejXARESF/feiXmtmLZva4mS0brZGZ3WxmjWbW2NHRkZc3Dq5UpC9ERUTyEehbgNPd/a3A14FHR2vo7ve7e4O7N9TW1k74jd09OKlIR+giIhMPdHc/4O7d4fJmIGFmNROuLAfJ3gF6+odYqCGLIiITD3QzO9XMLFxeGb5m50RfNxdt4QgXdbmIiOQwysXMHgauAGrMrBX4PJAAcPf1wBrgE2Y2CPQC17q7T1rFaY5cqUhdLiIiYwa6u183xvZvEAxrnHJtXbr0nIhISlGfKdqW7CNRatTOmlHoUkRECq64A72rl/mzKygpsUKXIiJScEUd6O1dulKRiEhKUQf6rq5eFmgOFxERoIgDfWjY2X1AR+giIilFG+gdBw8zOOzUKdBFRIAiDvS21Bh0dbmIiABFHOipedDV5SIiEijaQB85qUin/YuIAMUc6MleqspLmV2ZyzU6RESir3gDvauXujmVhPOCiYjEXtEGentSQxZFRNIVbaC36aQiEZGjFGWg9w0Msbe7X0foIiJpijLQ30ymLmyhI3QRkZSiDPSRk4p0hC4iMqI4A10nFYmIHKMoA709PKlIXS4iIkcUZaC3JfuYV1VORaK00KWIiEwbxRnoXb3U6cLQIiJHKcpAb0/2ag4XEZEMRRnobbr0nIjIMcYMdDN7wMz2mFnLKNvNzO4xsx1mttXMLsx/mUcc6Bug+/AgC9TlIiJylFyO0B8ErjrO9lXA0vB2M3DvxMsaXdvICBcdoYuIpBsz0N39KWDfcZpcAzzkgeeAOWZWl68CMx25sIWO0EVE0uWjD30h8Eba49Zw3THM7GYzazSzxo6OjhN6s5nlpbzjrFoWz515Qs8XEYmqfFwdItuE5J6tobvfD9wP0NDQkLXNWC4+Yx4XnzHvRJ4qIhJp+ThCbwUWpz1eBLTl4XVFRGQc8hHojwE3hKNdLgGS7t6eh9cVEZFxGLPLxcweBq4AasysFfg8kABw9/XAZuBqYAfQA9w4WcWKiMjoxgx0d79ujO0OfCpvFYmIyAkpyjNFRUTkWAp0EZGIUKCLiESEAl1EJCIU6CIiEaFAFxGJCAW6iEhEKNBFRCJCgS4iEhEKdBGRiFCgi4hEhAJdRCQiFOgiIhGhQBcRiQgFuohIRCjQRUQiQoEuIhIRCnQRkYhQoIuIRMSY1xSVIuAe3obHuGW0CZ4crD+h5RxrO3blCTxHJEIq58Ks2ry/rAJ9srhD/yE41AGH9ob34a1nHwz2wVD/kdtgavlw2nJq22EYGgi2DQ0E64aH0oJZAShSVC6/Da78Qt5fVoE+Xu7Q8Ts42HZsUKced4ePB3uzv0aiChIVUFp+5FY2I225HGbMgtIZUJoItyWCx2UzoKQsuFlJlpuNsj59e9gGC+oxO/HldJZlXbBhHG1FYqD27El52ZwC3cyuAr4GlAL/293vyth+BfBj4I/hqh+6+z/mr8xpYt8fYfPfw45fHr2+JAFVtVBVE9zPW3pkeeRWc+Q+UVmY+kWK1MDAAK2trfT19RW6lPzYD+zfftwmFRUVLFq0iEQikfPLjhnoZlYK/CtwJdAKvGBmj7n7yxlNf+Xuq3N+52Iy2A+/vgeeujsI7yv/JyxacSSgK07SEafIJGptbaW6upr6+nosBv/X3J3Ozk5aW1tZsmRJzs/L5Qh9JbDD3V8DMLPvA9cAmYEeTTt/DZv+FjpegbdcA1fdBbMXFLoqkVjp6+uLTZgDmBnz5s2jo6NjXM/LZdjiQuCNtMet4bpMl5rZi2b2uJktG6XIm82s0cwax1volOvZB49+Cv7PKhjogQ/9O3zgIYW5SIHEJcxTTuTnzeUIPdurZg6r2AKc7u7dZnY18Ciw9Jgnud8P3A/Q0NAwPYdmuEPz9+Dn/wMOHwi+jX7HP0D5zEJXJiJyXLkcobcCi9MeLwLa0hu4+wF37w6XNwMJM6vJW5VTpeP38OBq+PEnoeYs+JtfBUOLFOYisdbV1cU3v/lNAJ588klWrx7f14UPPvggbW1tYzecoFwC/QVgqZktMbNy4FrgsfQGZnaqhX8fmNnK8HU7813spBnohf/8Itx7GexugffcAzc+DvPfUujKRGQaSA/0EzFVgT5ml4u7D5rZrcDPCIYtPuDu28zslnD7emAN8AkzGwR6gWvdi+R0v1f/EzZ9Fvb/Ef7sWnj3FyflDC4RyY8v/GQbL7cdyOtrvmXBbD7/nqxf/QGwbt06Xn31VZYvX04ikaCqqoo1a9bQ0tLCRRddxHe/+13MjKamJj772c/S3d1NTU0NDz74IM888wyNjY1cf/31VFZW8uyzz3L33Xfzk5/8hN7eXi677DLuu+++vHxHkNNcLu6+2d3Pcvcz3f2fwnXrwzDH3b/h7svc/a3ufom7/3rClU22g7th48fgO++DklK44TF4/30KcxE5xl133cWZZ55Jc3Mzd999N7/97W/56le/yssvv8xrr73GM888w8DAAJ/+9KfZuHEjTU1N3HTTTXzuc59jzZo1NDQ0sGHDBpqbm6msrOTWW2/lhRdeoKWlhd7eXjZt2pSXOuN3pujwMDQ9AL/8x+D0+ytuh7fdFpyBKSLT3vGOpKfKypUrWbRoEQDLly/n9ddfZ86cObS0tHDllVcCMDQ0RF1dXdbnP/HEE3zpS1+ip6eHffv2sWzZMt7znvdMuK54Bbo7/HQdPH8fLHkHrP5fMO/MQlclIkVmxowjB4ClpaUMDg7i7ixbtoxnn332uM/t6+vjk5/8JI2NjSxevJg777wzb2fAxmv63KfuDsL80lvhhh8rzEUkJ9XV1Rw8ePC4bc4++2w6OjpGAn1gYIBt27Yd8/xUeNfU1NDd3c3GjRvzVmd8jtCf/zd44p9g+fXBF58xO0lBRE7cvHnzuPzyyznvvPOorKxk/vz5x7QpLy9n48aNrF27lmQyyeDgILfddhvLli3jox/9KLfccsvIl6If//jHOf/886mvr2fFihV5q9MKNRiloaHBGxsbp+bNXtoIj/w3OHsVfOA7UBqf32MiUbB9+3bOPffcQpcx5bL93GbW5O4N2dpHv8tlxy/hR7fA6ZfBmgcU5iISWdEO9DdegB98GE45B657WNPWikikRTfQ92yHDWug+lT4rz8MprgVEYmwaAb6/p3BCUNlFfDhH8GsUwpdkYjIpIteh3J3RxDmAz1w409hbn2hKxIRmRLRCvS+A/Dd98OBtmCcuSbXEpEYiU6Xy0AffP9DsOdl+OB34LSLC12RiETERGdbnCrRCPShQXjkY/D60/C++2DplYWuSEQipFgCvfi7XNxh02fglU2w6m44f02hKxKRyfT4Onjzpfy+5qnnw6q7Rt2cPn3uO9/5TrZu3cr+/fsZGBjgi1/8Itdccw2vv/46q1evpqWlBYAvf/nLdHd3c+edd+a31uMo/kD/5efht9+Fd6yDi28udDUiEkF33XUXLS0tNDc3Mzg4SE9PD7Nnz2bv3r1ccsklvPe97y10iUCxB/rTX4VnvgYrPg5XrCt0NSIyFY5zJD0V3J3bb7+dp556ipKSEnbt2sXu3bsLWlNK8Qb6lu8ER+fn/TWs+pIm2xKRKbFhwwY6OjpoamoikUhQX19PX18fZWVlDA8Pj7TL15S441GcX4pu3wQ/WQtn/iX81XooKc4fQ0SKQ/r0t8lkklNOOYVEIsETTzzBzp07AZg/fz579uyhs7OTw4cP5+0qRONRfEfof/wVbLwJFl4UDE8sKy90RSIScenT565YsYJXXnmFhoYGli9fzjnnnANAIpHgjjvu4OKLL2bJkiUj66dS8QV6VQ3UXw5//S0oryp0NSISE9/73vfGbLN27VrWrl07BdVkV3yBfsq5wfwsIiJyFHU+i4hERE6BbmZXmdnvzGyHmR0zPtAC94Tbt5rZhfkvVUTirFBXVyuUE/l5xwx0MysF/hVYBbwFuM7MMme9WgUsDW83A/eOuxIRkVFUVFTQ2dkZm1B3dzo7O6moqBjX83LpQ18J7HD31wDM7PvANcDLaW2uAR7yYG8/Z2ZzzKzO3dvHVY2ISBaLFi2itbWVjo6OQpcyZSoqKli0aNG4npNLoC8E3kh73ApkTmWYrc1C4KhAN7ObCY7gOe2008ZVqIjEVyKRYMmSJYUuY9rLpQ892ymYmX/35NIGd7/f3RvcvaG2tjaX+kREJEe5BHorsDjt8SKg7QTaiIjIJMol0F8AlprZEjMrB64FHsto8xhwQzja5RIgqf5zEZGpNWYfursPmtmtwM+AUuABd99mZreE29cDm4GrgR1AD3DjWK/b1NS018x2nmDdNcDeE3zuVJju9cH0r1H1TYzqm5jpXN/po22wYhwGZGaN7t5Q6DpGM93rg+lfo+qbGNU3MdO9vtHoTFERkYhQoIuIRESxBvr9hS5gDNO9Ppj+Naq+iVF9EzPd68uqKPvQRUTkWMV6hC4iIhkU6CIiETGtA306T9trZovN7Akz225m28zsM1naXGFmSTNrDm93TFV94fu/bmYvhe/dmGV7Ifff2Wn7pdnMDpjZbRltpnz/mdkDZrbHzFrS1p1sZr8wsz+E93NHee5xP6+TWN/dZvZK+G/4IzObM8pzj/t5mMT67jSzXWn/jleP8txC7b8fpNX2upk1j/LcSd9/E+bu0/JGcBLTq8AZQDnwIvCWjDZXA48TzCVzCfCbKayvDrgwXK4Gfp+lviuATQXch68DNcfZXrD9l+Xf+k3g9ELvP+DPgQuBlrR1XwLWhcvrgH8e5Wc47ud1Eut7N1AWLv9ztvpy+TxMYn13An+fw2egIPsvY/u/AHcUav9N9Dadj9BHpu11934gNW1vupFpe939OWCOmdVNRXHu3u7uW8Llg8B2ghkmi0nB9l+GvwRedfcTPXM4b9z9KWBfxuprgG+Hy98G/irLU3P5vE5Kfe7+c3cfDB8+RzCXUkGMsv9yUbD9l2JmBnwAeDjf7ztVpnOgjzYl73jbTDozqwcuAH6TZfOlZvaimT1uZsumtjIc+LmZNYVTF2eaFvuPYH6g0f4TFXL/pcz3cG6i8P6ULG2my768ieCvrmzG+jxMplvDLqEHRumymg777+3Abnf/wyjbC7n/cjKdAz1v0/ZOJjObBTwC3ObuBzI2byHoRngr8HXg0amsDbjc3S8kuKLUp8zszzO2T4f9Vw68F/j3LJsLvf/GYzrsy88Bg8CGUZqM9XmYLPcCZwLLCa6R8C9Z2hR8/wHXcfyj80Ltv5xN50Cf9tP2mlmCIMw3uPsPM7e7+wF37w6XNwMJM6uZqvrcvS283wP8iODP2nTTYdrjVcAWd9+duaHQ+y/N7lRXVHi/J0ubQn8WPwKsBq73sMM3Uw6fh0nh7rvdfcjdh4F/G+V9C73/yoD3Az8YrU2h9t94TOdAn9bT9ob9bd8Ctrv7V0Zpc2rYDjNbSbC/O6eoviozq04tE3xx1pLRbDpMezzqUVEh91+Gx4CPhMsfAX6cpU0un9dJYWZXAf8AvNfde0Zpk8vnYbLqS/9e5n2jvG/B9l/oXcAr7t6abWMh99+4FPpb2ePdCEZh/J7g2+/PhetuAW4Jl43gAtavAi8BDVNY29sI/iTcCjSHt6sz6rsV2Ebwjf1zwGVTWN8Z4fu+GNYwrfZf+P4zCQL6pLR1Bd1/BL9c2oEBgqPGjwHzgP8A/hDenxy2XQBsPt7ndYrq20HQ/5z6HK7PrG+0z8MU1fed8PO1lSCk66bT/gvXP5j63KW1nfL9N9GbTv0XEYmI6dzlIiIi46BAFxGJCAW6iEhEKNBFRCJCgS4iEhEKdCl6ZtYd3teb2Yfy/Nq3Zzz+dT5fXySfFOgSJfXAuALdzErHaHJUoLv7ZeOsSWTKKNAlSu4C3h7OV/23ZlYazhX+Qjgx1N/AyDzrT5jZ9whOeMHMHg0nXdqWmnjJzO4CKsPX2xCuS/01YOFrt4RzZH8w7bWfNLONFsxRviF1tqvIZCsrdAEiebSOYN7t1QBhMCfdfYWZzQCeMbOfh21XAue5+x/Dxze5+z4zqwReMLNH3H2dmd3q7suzvNf7CSabeitQEz7nqXDbBcAygrlIngEuB57O9w8rkklH6BJl7yaYq6aZYGrjecDScNvzaWEOsNbMUlMMLE5rN5q3AQ97MOnUbuD/ASvSXrvVg8momgm6gkQmnY7QJcoM+LS7/+yolWZXAIcyHr8LuNTde8zsSaAih9cezeG05SH0/0ymiI7QJUoOElwOMOVnwCfCaY4xs7PCmfIynQTsD8P8HILL8aUMpJ6f4Sngg2E/fS3Bpc2ez8tPIXKCdOQgUbIVGAy7Th4EvkbQ3bEl/GKyg+yXj/spcIuZbQV+R9DtknI/sNXMtrj79WnrfwRcSjD7ngP/3d3fDH8hiBSEZlsUEYkIdbmIiESEAl1EJCIU6CIiEaFAFxGJCAW6iEhEKNBFRCJCgS4iEhH/H25Ij0aHQyD3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# template for Problem 2(b)\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Simulate data:\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "theta_true = 3\n",
    "tau_true = 0.5\n",
    "n_samples = 100\n",
    "\n",
    "x = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    # Sample from N(0,1) or N(theta_true,1)\n",
    "    if np.random.rand() < 1 - tau_true:\n",
    "        x[i] = np.random.normal(0, 1)\n",
    "    else:\n",
    "        x[i] = np.random.normal(theta_true, 1)\n",
    "\n",
    "\n",
    "### The EM algorithm:\n",
    "\n",
    "n_iter = 20\n",
    "theta = np.zeros(n_iter)\n",
    "tau = np.zeros(n_iter)\n",
    "\n",
    "# Initial guesses for theta and tau\n",
    "theta[0] = 1\n",
    "tau[0] = 0.1\n",
    "\n",
    "for it in range(1, n_iter):\n",
    "    # The current estimates for theta and tau,\n",
    "    # computed in the previous iteration\n",
    "    theta_0 = theta[it-1]\n",
    "    tau_0 = tau[it-1]\n",
    "\n",
    "    # E-step: compute the responsibilities r1 and r2\n",
    "    # r1 = ?\n",
    "    # r2 = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    r1_unnorm=(1-tau_0)*scipy.stats.norm.pdf(x,0,1)\n",
    "    r2_unnorm=tau_0*scipy.stats.norm.pdf(x,theta_0,1)\n",
    "    r1=r1_unnorm/(r1_unnorm+r2_unnorm)\n",
    "    r2=r2_unnorm/(r1_unnorm+r2_unnorm)\n",
    "    # M-step: compute the parameter values that maximize\n",
    "    # the expectation of the complete-data log-likelihood.\n",
    "    # theta[it] = ?\n",
    "    # tau[it] = ?\n",
    "    theta[it]=sum(r2*x)/sum(r2)\n",
    "    tau[it]=np.mean(r2)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "\n",
    "# Print and plot the values of theta and tau in each iteration\n",
    "print(\"theta       tau\")\n",
    "for theta_i, tau_i in zip(theta, tau):\n",
    "    print(\"{0:.7f}  {1:.7f}\".format(theta_i, tau_i))\n",
    "\n",
    "plt.plot(range(n_iter), theta, label = 'theta')\n",
    "plt.plot(range(n_iter), tau, label = 'tau')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82083e7d021d449db017e6010251e7ad",
     "grade": false,
     "grade_id": "cell-482274cb8fbd6887",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Problem 3: PyTorch\n",
    "Go through the PyTorch tutorials in the three links and answer the questions given below\n",
    "\n",
    "1) What is PyTorch: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "2) Autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "3) Linear regression with PyTorch: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "\n",
    "__(a)__ What are PyTorch Tensors and how do you run a CPU tensor on GPU? \n",
    "\n",
    "\n",
    "__(b)__ What is Automatic differentiation and autograd? \n",
    "\n",
    "\n",
    "__(c)__ PyTorch constructs the computation graph dynamically as the operations are defined. In the 'linear regression with PyTorch' tutorial which line numbers indicates the completion of the computation graph, computation of the gradients and update of the weights, respectively? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "Tensors are very similar to NumPy’s ndarrays, except that tensors can run on **GPUs** or other specialized hardware to accelerate computing.\n",
    "\n",
    "We can allocate a GPU in Colab:\n",
    "```\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "```\n",
    "\n",
    "### b)\n",
    "**Automatic differentiation** is the technique for the evaluation of the derivatives of a function automatically based on neural networks.\n",
    "\n",
    "Autograd is PyTorch’s automatic differentiation engine that powers neural network training. Generally speaking, autograd is an engine for computing vector-Jacobian product $\n",
    "J^{T} \\cdot {\\overrightarrow{v }}\n",
    "$.\n",
    "\n",
    "### c)\n",
    "**The completion of the computation graph:** lines 23\n",
    "\n",
    "**The computation of the gradients:** line 41\n",
    "\n",
    "**The update of the weights:** line 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
